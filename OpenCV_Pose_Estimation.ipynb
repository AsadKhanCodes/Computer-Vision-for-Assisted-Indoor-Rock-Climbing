{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.10.7)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.8.1.78)\n",
      "Requirement already satisfied: absl-py in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (2.0.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (3.8.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (1.26.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->mediapipe) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ultralytics in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (8.0.203)\n",
      "Requirement already satisfied: supervision in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.16.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.8.0.74 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from supervision) (4.8.1.78)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lauramaldonado/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mediapipe opencv-python\n",
    "%pip install ultralytics supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uuid\n",
    "# import os\n",
    "# mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/3j8BPdc.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qpRACer.png\" style=\"height:300px\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ldmrk in mp_hands.HandLandmark:\n",
    "#     print(ldmrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_hand(image, hand_pts):\n",
    "    # DISPLAY AREA OF RIGHT HAND ---------------\n",
    "    # Calculate the center of the circle in 3D space (x, y, z)\n",
    "    center_3d = np.mean(hand_pts, axis=0)\n",
    "    # Calculate the radius of the circle in 3D space based on the average distance from the center to each point\n",
    "    distances = [np.linalg.norm([p[0] - center_3d[0], p[1] - center_3d[1], p[2] - center_3d[2]]) for p in hand_pts]\n",
    "\n",
    "    scaling_factor = 3  # scaling factor must be int\n",
    "    radius_3d = scaling_factor * int(sum(distances ) / len(distances))\n",
    "\n",
    "    # Draw the circle in 3D space\n",
    "    cv2.circle(image, (int(center_3d[0]), int(center_3d[1])), radius_3d, (245, 117, 66), thickness=-1)  # -1 thickness for a filled circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feet_pts(ankle, heel, index, frame_shape_1, frame_shape_0):\n",
    "    return np.array([\n",
    "        [int(ankle.x * frame_shape_1), int(ankle.y * frame_shape_0)],\n",
    "        [int(heel.x * frame_shape_1), int(heel.y * frame_shape_0)],\n",
    "        [int(index.x * frame_shape_1), int(index.y * frame_shape_0)]\n",
    "        ], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hand_pts(pinky, index, thumb, wrist, frame_shape_1, frame_shape_0):\n",
    "    return np.array([\n",
    "        [int(pinky.x * frame_shape_1), int(pinky.y * frame_shape_0)],\n",
    "        [int(index.x * frame_shape_1), int(index.y * frame_shape_0)],\n",
    "        [int(thumb.x * frame_shape_1), int(thumb.y * frame_shape_0)],\n",
    "        [int(wrist.x * frame_shape_1), int(wrist.y * frame_shape_0)]\n",
    "        ], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_coords(d):\n",
    "    max_key_length = max(len(key) for key in d.keys())\n",
    "    max_x_length = max(len(str(value.x)) for value in d.values())\n",
    "    max_y_length = max(len(str(value.y)) for value in d.values())\n",
    "    max_z_length = max(len(str(value.z)) for value in d.values())\n",
    "\n",
    "    for point, coords in d.items():\n",
    "        formatted_x = str(coords.x).rjust(max_x_length)\n",
    "        formatted_y = str(coords.y).rjust(max_y_length)\n",
    "        formatted_z = str(coords.z).rjust(max_z_length)\n",
    "        print(f\"{point.ljust(max_key_length)}: x = {formatted_x}, y = {formatted_y}, z = {formatted_z}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_hold(limb, detection):\n",
    "    x, y = limb.x, limb.y\n",
    "    x1, y1, x2, y2 = detection[0], detection[1], detection[2], detection[3]\n",
    "    # Check if limb coordinates are within the bounding box\n",
    "    return x1 <= x <= x2 and y1 <= y <= y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_point(d, limb, right_foot_pts, left_foot_pts, right_hand_pts, left_hand_pts):\n",
    "    r_foot = [\"right_ankle\", \"right_heel\", \"right_foot_index\"]\n",
    "    l_foot = [\"left_ankle\", \"left_heel\", \"left_foot_index\"]\n",
    "    r_hand = [\"right_pinky\", \"right_index\",\"right_thumb\", \"right_wrist\"]\n",
    "    l_hand = [\"left_pinky\", \"left_index\",\"left_thumb\", \"left_wrist\"]\n",
    "    if limb in r_foot:\n",
    "        return np.mean(right_foot_pts, axis=0)\n",
    "    elif limb in l_foot:\n",
    "        return np.mean(left_foot_pts, axis=0)\n",
    "    elif limb in r_hand:\n",
    "        return np.mean(right_hand_pts, axis=0)\n",
    "    elif limb in l_hand:\n",
    "        return np.mean(left_hand_pts, axis=0)\n",
    "    else:\n",
    "        return np.array([d[limb].x, d[limb].y], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: function that checks what holds the person is on\n",
    "# A hold corresponding to right hand, left hand, right foot, left foot\n",
    "def get_curr_position(d, detections):\n",
    "    extremities = [\"right_foot\", \"left_foot\", \"right_hand\",\"left_hand\"]\n",
    "    for limb, coords in d.items():\n",
    "        for detection in detections:\n",
    "            # yields opposite corners\n",
    "            x1, y1, x2, y2 = detection[0], detection[1], detection[2], detection[3]\n",
    "            limb_x, limb_y = coords.x, coords.y\n",
    "\n",
    "            if (limb in extremities) and x1 <= limb_x <= x2 and y1 <= limb_y <= y2: # Within bounds\n",
    "                # save the coordinates\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_position(center_limb_pt, rock_hold):\n",
    "    # points of rock_hold\n",
    "    x1, y1, x2, y2 = rock_hold[0][0], rock_hold[0][1], rock_hold[0][2], rock_hold[0][3]\n",
    "    # print(\"Rock_coords:\", x1, y1, x2, y2)\n",
    "    mean_rock_coord = np.mean(np.array([[x1, y1], [x2, y2]]), axis=0)\n",
    "    # print(\"M:\", mean_rock_coord)\n",
    "    # print(\"C:\", center_limb_pt[:2])\n",
    "    return np.linalg.norm(abs(center_limb_pt[:2] - mean_rock_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative position: 400.98041654115764                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39mprocess(image)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# YOLO results\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m results2 \u001b[39m=\u001b[39m model(frame, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb#X23sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m detections \u001b[39m=\u001b[39m sv\u001b[39m.\u001b[39mDetections\u001b[39m.\u001b[39mfrom_ultralytics(results2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lauramaldonado/Documents/CSC492/Computer-Science-Expository-Work/OpenCV_Pose_Estimation.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m detections \u001b[39m=\u001b[39m detections[detections\u001b[39m.\u001b[39mconfidence \u001b[39m>\u001b[39m \u001b[39m0.8\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/engine/model.py:101\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    100\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/engine/model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m prompts \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor, \u001b[39m'\u001b[39m\u001b[39mset_prompts\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 242\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39mpredict_cli(source\u001b[39m=\u001b[39msource) \u001b[39mif\u001b[39;00m is_cli \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor(source\u001b[39m=\u001b[39;49msource, stream\u001b[39m=\u001b[39;49mstream)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/engine/predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_inference(source, model, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_inference(source, model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/engine/predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 259\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minference(im, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[39m# Postprocess\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mwith\u001b[39;00m profilers[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/engine/predictor.py:135\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m visualize \u001b[39m=\u001b[39m increment_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_dir \u001b[39m/\u001b[39m Path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mstem,\n\u001b[1;32m    134\u001b[0m                            mkdir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mvisualize \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type\u001b[39m.\u001b[39mtensor) \u001b[39melse\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:347\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    344\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_module:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    348\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 59\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 79\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     80\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/modules/block.py:202\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    201\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    203\u001b[0m     y\u001b[39m.\u001b[39mextend(m(y[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm)\n\u001b[1;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ultralytics/nn/modules/conv.py:40\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# JUST THE POSE\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "model = YOLO('bestHuge.pt')\n",
    "box_annotator = sv.BoxAnnotator(thickness=2, text_thickness=2, text_scale=1)\n",
    "\n",
    "## Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        # Recolor image to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "\n",
    "\n",
    "        # YOLO results\n",
    "        results2 = model(frame, verbose=False)[0]\n",
    "        detections = sv.Detections.from_ultralytics(results2)\n",
    "        detections = detections[detections.confidence > 0.8]\n",
    "\n",
    "        frame = box_annotator.annotate(scene=image, detections=detections)\n",
    "\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "        # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "        #                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            pose_landmark = mp_pose.PoseLandmark\n",
    "\n",
    "            d = {}  # body dictionary\n",
    "\n",
    "            # Upper body coordinates\n",
    "            d[\"left_shoulder\"] = landmarks[pose_landmark.LEFT_SHOULDER.value]\n",
    "            d[\"right_shoulder\"] = landmarks[pose_landmark.RIGHT_SHOULDER]\n",
    "\n",
    "            d[\"left_elbow\"] = landmarks[pose_landmark.LEFT_ELBOW.value]\n",
    "            d[\"right_elbow\"] = landmarks[pose_landmark.RIGHT_ELBOW.value]\n",
    "\n",
    "            frame_shape_0, frame_shape_1 = frame.shape[0], frame.shape[1]\n",
    "\n",
    "            # LEFT HAND\n",
    "            d[\"left_pinky\"] = landmarks[pose_landmark.LEFT_PINKY.value]\n",
    "            d[\"left_index\"] = landmarks[pose_landmark.LEFT_INDEX.value]\n",
    "            d[\"left_thumb\"] = landmarks[pose_landmark.LEFT_THUMB.value]\n",
    "            d[\"left_wrist\"] = landmarks[pose_landmark.LEFT_WRIST.value]\n",
    "            left_hand_pts = hand_pts(d[\"left_pinky\"], d[\"left_index\"], d[\"left_thumb\"], d[\"left_wrist\"], frame_shape_1, frame_shape_0)\n",
    "\n",
    "            # RIGHT HAND\n",
    "            d[\"right_pinky\"] = landmarks[pose_landmark.RIGHT_PINKY.value]\n",
    "            d[\"right_index\"] = landmarks[pose_landmark.RIGHT_INDEX.value]\n",
    "            d[\"right_thumb\"] = landmarks[pose_landmark.RIGHT_THUMB.value]\n",
    "            d[\"right_wrist\"] = landmarks[pose_landmark.RIGHT_WRIST.value]\n",
    "            right_hand_pts = hand_pts(d[\"right_pinky\"], d[\"right_index\"], d[\"right_thumb\"], d[\"right_wrist\"], frame_shape_1, frame_shape_0)\n",
    "\n",
    "            # Lower body coordinates\n",
    "            d[\"left_knee\"] = landmarks[pose_landmark.LEFT_KNEE.value]\n",
    "            d[\"right_knee\"] = landmarks[pose_landmark.RIGHT_KNEE.value]\n",
    "\n",
    "            # LEFT FOOT\n",
    "            d[\"left_ankle\"] = landmarks[pose_landmark.LEFT_ANKLE.value]\n",
    "            d[\"left_heel\"] = landmarks[pose_landmark.LEFT_HEEL.value]\n",
    "            d[\"left_foot_index\"] = landmarks[pose_landmark.LEFT_FOOT_INDEX.value]\n",
    "            left_foot_pts = feet_pts(d[\"left_ankle\"], d[\"left_heel\"], d[\"left_foot_index\"], frame_shape_1, frame_shape_0)\n",
    "\n",
    "            # RIGHT FOOT\n",
    "            d[\"right_ankle\"] = landmarks[pose_landmark.RIGHT_ANKLE.value]\n",
    "            d[\"right_heel\"] = landmarks[pose_landmark.RIGHT_HEEL.value]\n",
    "            d[\"right_foot_index\"] = landmarks[pose_landmark.RIGHT_FOOT_INDEX.value]\n",
    "            right_foot_pts = feet_pts(d[\"right_ankle\"], d[\"right_heel\"], d[\"right_foot_index\"], frame_shape_1, frame_shape_0)\n",
    "\n",
    "            # Display Coordinates\n",
    "            # display_coords(d)\n",
    "\n",
    "            for detection in detections:\n",
    "                # currently only for right_hand\n",
    "                point = get_center_point(d, \"right_thumb\", right_foot_pts, left_foot_pts, right_hand_pts, left_hand_pts)\n",
    "                get_relative_position(point, detection)\n",
    "                print(f\"Relative position: {get_relative_position(point, detection)} \" + \" \" * 20, end='\\r')\n",
    "\n",
    "            # center_2d = np.mean(right_hand_pts, axis=0)[:2]\n",
    "\n",
    "            cv2.fillPoly(image, [right_foot_pts], (245, 117, 66))\n",
    "            cv2.fillPoly(image, [left_foot_pts], (245, 117, 66))\n",
    "\n",
    "            display_hand(image,right_hand_pts)\n",
    "            display_hand(image,left_hand_pts)\n",
    "\n",
    "            # Calculate angle\n",
    "            # angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            # Visualize angle\n",
    "            # cv2.putText(image, str(angle),\n",
    "            #                tuple(np.multiply(elbow, [640, 480]).astype(int)),\n",
    "            #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "            #                     )\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(results.pose_landmarks)\n",
    "        # Render detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "        #                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "        #                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Pose Detection', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUST THE HANDS\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands: \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # BGR 2 RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Flip on horizontal\n",
    "        image = cv2.flip(image, 1)\n",
    "        \n",
    "        # Set flag\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Detections\n",
    "        results = hands.process(image)\n",
    "        \n",
    "        # Set flag to true\n",
    "        image.flags.writeable = True\n",
    "        \n",
    "        # RGB 2 BGR\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Detections\n",
    "        print(results)\n",
    "        \n",
    "        # Rendering results\n",
    "        if results.multi_hand_landmarks:\n",
    "            for num, hand in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(image, hand, mp_hands.HAND_CONNECTIONS, \n",
    "                                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
    "                                         )\n",
    "            \n",
    "        # Save our image    \n",
    "        cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE HANDS AND THE POSE\n",
    "# TODO: REMOVE THE mini hands from the pose\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Setup mediapipe instance\n",
    "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose, mp_hands.Hands(min_detection_confidence=0.3, min_tracking_confidence=0.3) as hands:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        # Get image used for pose estimation\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make detection\n",
    "        results = pose.process(image)\n",
    "        hand_results = hands.process(image)\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            hand_landmarks = hand_results.multi_hand_landmarks\n",
    "\n",
    "            # PRINTING THE POSE COORDINATES\n",
    "            left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "            right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER]\n",
    "            print(\"SHOULDERS\")\n",
    "            print(\"Left_Shoulder --- x: \" + str(left_shoulder.x) + \"y: \" + str(left_shoulder.y) +  \"z: \" + str(left_shoulder.z))\n",
    "            print(\"Right_Shoulder --- x: \" + str(right_shoulder.x) + \"y: \" + str(right_shoulder.y) + \"z: \" + str(right_shoulder.z))\n",
    "\n",
    "            print(\"ELBOWS\")\n",
    "            left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "            right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "            print(\"Left_elbow --- x: \" + str(left_elbow.x) + \"y: \" + str(left_elbow.y) + \"z: \" + str(left_elbow.z))\n",
    "            print(\"Right_elbow --- x: \" + str(right_elbow.x) + \"y: \" + str(right_elbow.y) + \"z: \" + str(right_elbow.z))\n",
    "\n",
    "            # LEFT HAND\n",
    "            left_pinky = landmarks[mp_pose.PoseLandmark.LEFT_PINKY.value]\n",
    "            left_index = landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value]\n",
    "            left_thumb = landmarks[mp_pose.PoseLandmark.LEFT_THUMB.value]\n",
    "            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "            print(\"LEFT HAND\")\n",
    "            print(\"Left_pinky --- x: \" + str(left_pinky.x) + \"y: \" + str(left_pinky.y) + \"z: \" + str(left_pinky.z))\n",
    "            print(\"left_index --- x: \" + str(left_index.x) + \"y: \" + str(left_index.y) + \"z: \" + str(left_index.z))\n",
    "            print(\"left_thumb --- x: \" + str(left_thumb.x) + \"y: \" + str(left_thumb.y) + \"z: \" + str(left_thumb.z))\n",
    "            print(\"Left_wrist --- x: \" + str(left_wrist.x) + \"y: \" + str(left_wrist.y) + \"z: \" + str(left_wrist.z))\n",
    "\n",
    "            # RIGHT HAND\n",
    "            right_pinky = landmarks[mp_pose.PoseLandmark.RIGHT_PINKY.value]\n",
    "            right_index = landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value]\n",
    "            right_thumb = landmarks[mp_pose.PoseLandmark.RIGHT_THUMB.value]\n",
    "            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n",
    "            print(\"RIGHT HAND\")\n",
    "            print(\"Right_pinky --- x: \" + str(right_pinky.x) + \"y: \" + str(right_pinky.y) + \"z: \" + str(right_pinky.z))\n",
    "            print(\"Right_index --- x: \" + str(right_index.x) + \"y: \" + str(right_index.y) + \"z: \" + str(right_index.z))\n",
    "            print(\"Right_thumb --- x: \" + str(right_thumb.x) + \"y: \" + str(right_thumb.y) + \"z: \" + str(right_thumb.z))\n",
    "            print(\"Right_wrist: x: \" + str(right_wrist.x) + \"y: \" + str(right_wrist.y) + \"z: \" + str(right_wrist.z))\n",
    "\n",
    "            # Lower body coordinates\n",
    "            left_knee = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value]\n",
    "            print(\"LEFT KNEE --- x: \" + str(left_knee.x) + \" y: \" + str(left_knee.y) + \"z: \" + str(left_knee.z))\n",
    "\n",
    "            right_knee = landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value]\n",
    "            print(\"RIGHT KNEE --- x: \" + str(right_knee.x) + \" y: \" + str(right_knee.y) + \"z: \" + str(right_knee.z))\n",
    "\n",
    "            # LEFT FOOT\n",
    "            left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value]\n",
    "            left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value]\n",
    "            left_foot_index = landmarks[mp_pose.PoseLandmark.LEFT_FOOT_INDEX.value]\n",
    "            print(\"LEFT FOOT\")\n",
    "            print(\"Left_ankle --- x: \" + str(left_ankle.x) + \" y: \" + str(left_ankle.y) + \" z: \" + str(left_ankle.z))\n",
    "            print(\"Left_heel --- x: \" + str(left_heel.x) + \" y: \" + str(left_heel.y) + \" z: \" + str(left_heel.z))\n",
    "            print(\"Left_foot_index --- x: \" + str(left_foot_index.x) + \" y: \" + str(left_foot_index.y) + \" z: \" + str(left_foot_index.z))\n",
    "\n",
    "            # RIGHT FOOT\n",
    "            right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value]\n",
    "            right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value]\n",
    "            right_foot_index = landmarks[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX.value]\n",
    "            print(\"RIGHT FOOT\")\n",
    "            print(\"Right_ankle --- x: \" + str(right_ankle.x) + \" y: \" + str(right_ankle.y) + \" z: \" + str(right_ankle.z))\n",
    "            print(\"Right_heel --- x: \" + str(right_heel.x) + \" y: \" + str(right_heel.y) + \" z: \" + str(right_heel.z))\n",
    "            print(\"Right_foot_index --- x: \" + str(right_foot_index.x) + \" y: \" + str(right_foot_index.y) + \" z: \" + str(right_foot_index.z))\n",
    "\n",
    "            frame_shape_0, frame_shape_1 = frame.shape[0], frame.shape[1]\n",
    "\n",
    "            # DISPLAY AREA OF RIGHT FOOT ---------------\n",
    "            right_foot_pts = np.array([\n",
    "                [int(right_ankle.x * frame_shape_1), int(right_ankle.y * frame_shape_0)],\n",
    "                [int(right_heel.x * frame_shape_1), int(right_heel.y * frame_shape_0)],\n",
    "                [int(right_foot_index.x * frame_shape_1), int(right_foot_index.y * frame_shape_0)]\n",
    "            ], np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            # Fill the area of the right foot with a specific color\n",
    "            cv2.fillPoly(image, [right_foot_pts], (245, 117, 66))\n",
    "\n",
    "            # DISPLAY AREA OF LEFT FOOT ---------------\n",
    "            left_foot_pts = np.array([\n",
    "                [int(left_ankle.x * frame_shape_1), int(left_ankle.y * frame_shape_0)],\n",
    "                [int(left_heel.x * frame_shape_1), int(left_heel.y * frame_shape_0)],\n",
    "                [int(left_foot_index.x * frame_shape_1), int(left_foot_index.y * frame_shape_0)]\n",
    "            ], np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            # Fill the area of the left foot with a specific color\n",
    "            cv2.fillPoly(image, [left_foot_pts], (245, 117, 66))\n",
    "    \n",
    "            # DISPLAY AREA OF RIGHT HAND ---------------\n",
    "            right_hand_pts = np.array([\n",
    "                [int(right_pinky.x * frame_shape_1), int(right_pinky.y * frame_shape_0), right_pinky.z],\n",
    "                [int(right_index.x * frame_shape_1), int(right_index.y * frame_shape_0), right_index.z],\n",
    "                [int(right_thumb.x * frame_shape_1), int(right_thumb.y * frame_shape_0), right_thumb.z],\n",
    "                [int(right_wrist.x * frame_shape_1), int(right_wrist.y * frame_shape_0), right_wrist.z]\n",
    "            ], np.int32)\n",
    "\n",
    "            # Calculate the center of the circle in 3D space (x, y, z)\n",
    "            right_center_3d = np.mean(right_hand_pts, axis=0)\n",
    "\n",
    "            # Calculate the radius of the circle in 3D space based on the average distance from the center to each point\n",
    "            right_distances = [np.linalg.norm([p[0] - right_center_3d[0], p[1] - right_center_3d[1], p[2] - right_center_3d[2]]) for p in right_hand_pts]\n",
    "            radius_3d = int(sum(right_distances ) / len(right_distances))\n",
    "\n",
    "            # Draw the circle in 3D space\n",
    "            cv2.circle(image, (int(right_center_3d[0]), int(right_center_3d[1])), radius_3d, (245, 117, 66), thickness=-1)  # -1 thickness for a filled circle\n",
    "\n",
    "            # DISPLAY AREA OF LEFT HAND ---------------\n",
    "            left_hand_pts = np.array([\n",
    "                [int(left_pinky.x * frame_shape_1), int(left_pinky.y * frame_shape_0), left_pinky.z],\n",
    "                [int(left_index.x * frame_shape_1), int(left_index.y * frame_shape_0), left_index.z],\n",
    "                [int(left_thumb.x * frame_shape_1), int(left_thumb.y * frame_shape_0), left_thumb.z],\n",
    "                [int(left_wrist.x * frame_shape_1), int(left_wrist.y * frame_shape_0), left_wrist.z]\n",
    "            ], np.int32)\n",
    "\n",
    "            # Calculate the center of the circle in 3D space (x, y, z)\n",
    "            left_center_3d = np.mean(left_hand_pts, axis=0)\n",
    "\n",
    "            # Calculate the radius of the circle in 3D space based on the average distance from the center to each point\n",
    "            left_distances = [np.linalg.norm([p[0] - left_center_3d[0], p[1] - left_center_3d[1], p[2] - left_center_3d[2]]) for p in left_hand_pts]\n",
    "            radius_3d = int(sum(left_distances ) / len(left_distances))\n",
    "\n",
    "            # Draw the circle in 3D space\n",
    "            cv2.circle(image, (int(left_center_3d[0]), int(left_center_3d[1])), radius_3d, (245, 117, 66), thickness=-1)  # -1 thickness for a filled circle\n",
    "\n",
    "            # Calculate angle\n",
    "            # angle = calculate_angle(shoulder, elbow, wrist)\n",
    "            # Visualize angle\n",
    "            # cv2.putText(image, str(angle),\n",
    "            #                tuple(np.multiply(elbow, [640, 480]).astype(int)),\n",
    "            #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA\n",
    "            #                     )\n",
    "\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                            mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                            mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "            \n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # print(results.pose_landmarks)\n",
    "        # Render detections\n",
    "\n",
    "        cv2.imshow('Pose Detection', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
